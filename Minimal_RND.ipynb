{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPKrf_qQf3M_",
        "outputId": "9fcd54ad-65ce-46f9-f59f-f0122a1ef21e"
      },
      "outputs": [],
      "source": [
        "!pip install renderlab  # library used for rendering gym envs on colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SyMEGmvcL_A",
        "outputId": "122c971b-acb5-43c9-8497-7188e9f27b0f"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade sympy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lg6njoVVf81L"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import renderlab as rl\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical, Normal\n",
        "from torch.optim.adam import Adam\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# config = {\n",
        "\n",
        "#         \"learning_rate\" : 0.0002,\n",
        "#         \"gamma\" : 0.98,           # reward discount factor\n",
        "#         \"lmbda\" : 0.95,           # for GAE in PPOtorch\n",
        "#         \"eps_clip\" : 0.2,         # for ratio clipping in PPO\n",
        "#         \"K_epoch\" : 10,            # how many times you repeatedly reuse your data\n",
        "#         \"T_horizon\" : 20,         # number of transition in single minibatch\n",
        "#         \"n_states\" : 4+1,         # state space. Additional one dimension for [time]\n",
        "#         \"n_actions\": 2,           # action space\n",
        "#         \"n_skills\" : 1,           # In DIAYN, dimension of discrete skills\n",
        "#         \"n_train_episode\" : 1000, # total number of episodes for training\n",
        "#         \"print_interval\" : 10,    # to see the training progress\n",
        "#         \"entropy_coeff\": 0.1,\n",
        "#     }\n",
        "\n",
        "config = {\n",
        "    'learning_rate'  : 0.0003,\n",
        "    'gamma'           : 0.9,\n",
        "    'lmbda'           : 0.9,\n",
        "    'eps_clip'        : 0.2,\n",
        "    'K_epoch'         : 10,\n",
        "    'rollout_len'    : 3,\n",
        "    'buffer_size'    : 10,\n",
        "    'minibatch_size' : 32,\n",
        "    \"entropy_coeff\": 0.001,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T7UeRUh2f9Rz",
        "outputId": "4d233c46-5a22-47c0-d073-0d82153f719c"
      },
      "outputs": [],
      "source": [
        "class PPO(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(PPO, self).__init__()\n",
        "        self.data = []\n",
        "        self.entropy_coeff = config['entropy_coeff']\n",
        "        self.learning_rate = config['learning_rate']\n",
        "        self.fc1   = nn.Linear(3,128)\n",
        "        self.fc_mu = nn.Linear(128,1)\n",
        "        self.fc_std  = nn.Linear(128,1)\n",
        "        self.fc_v = nn.Linear(128,1)\n",
        "        self.fc_dv = nn.Linear(128,1)\n",
        "        self.target_network = nn.Sequential(nn.Linear(3,128), nn.ReLU(), nn.Linear(128,1))\n",
        "        self.prediction_network = nn.Sequential(nn.Linear(3,128), nn.ReLU(), nn.Linear(128,1))\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        self.optimization_step = 0\n",
        "        self.buffer_size = config['buffer_size']\n",
        "        self.minibatch_size = config['minibatch_size']\n",
        "        self.rollout_len = config['rollout_len']\n",
        "        self.eps_clip = config['eps_clip']\n",
        "        self.gamma = config['gamma']\n",
        "        self.lmbda = config['lmbda']\n",
        "        self.K_epoch = config['K_epoch']\n",
        "\n",
        "    def pi(self, x, softmax_dim = 0):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        mu = 2.0*torch.tanh(self.fc_mu(x))\n",
        "        std = F.softplus(self.fc_std(x))\n",
        "        return mu, std\n",
        "\n",
        "    def v(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        v = self.fc_v(x)\n",
        "\n",
        "        return v\n",
        "    def dv(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        dv = self.fc_dv(x)\n",
        "\n",
        "        return dv\n",
        "    def put_data(self, transition):\n",
        "        self.data.append(transition)\n",
        "\n",
        "    def make_batch(self):\n",
        "        s_batch, a_batch, r_batch, rnd_batch, s_prime_batch, prob_a_batch, done_batch = [], [], [], [], [], [],[]\n",
        "        data = []\n",
        "\n",
        "        for j in range(self.buffer_size):\n",
        "            for i in range(self.minibatch_size):\n",
        "                rollout = self.data.pop()\n",
        "                s_lst, a_lst, r_lst, rnd_lst,s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], [], []\n",
        "\n",
        "                for transition in rollout:\n",
        "                    s, a, r, s_prime, prob_a, done = transition\n",
        "\n",
        "                    s_lst.append(s)\n",
        "                    a_lst.append([a])\n",
        "                    r_lst.append([r[0]])\n",
        "                    rnd_lst.append([r[1]])\n",
        "                    s_prime_lst.append(s_prime)\n",
        "                    prob_a_lst.append([prob_a])\n",
        "                    done_mask = 0 if done else 1\n",
        "                    done_lst.append([done_mask])\n",
        "\n",
        "                s_batch.append(s_lst)\n",
        "                a_batch.append(a_lst)\n",
        "                r_batch.append(r_lst)\n",
        "                rnd_batch.append(rnd_lst)\n",
        "                s_prime_batch.append(s_prime_lst)\n",
        "                prob_a_batch.append(prob_a_lst)\n",
        "                done_batch.append(done_lst)\n",
        "\n",
        "            mini_batch = torch.tensor(s_batch, dtype=torch.float), torch.tensor(a_batch, dtype=torch.float), \\\n",
        "                          torch.tensor(r_batch, dtype=torch.float), torch.tensor(rnd_batch, dtype=torch.float),\\\n",
        "                          torch.tensor(s_prime_batch, dtype=torch.float), \\\n",
        "                          torch.tensor(done_batch, dtype=torch.float), torch.tensor(prob_a_batch, dtype=torch.float)\n",
        "            data.append(mini_batch)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def calc_advantage(self, data):\n",
        "        data_with_adv = []\n",
        "        for mini_batch in data:\n",
        "            s, a, r, rnd_r,s_prime, done_mask, old_log_prob = mini_batch\n",
        "            with torch.no_grad():\n",
        "                # print(self.gamma)\n",
        "                # print(r[0].shape)\n",
        "                # print(self.v(s_prime).shape)\n",
        "                # print(done_mask.shape)\n",
        "                td_target = r + self.gamma * self.v(s_prime) * done_mask\n",
        "                delta = td_target - self.v(s)\n",
        "                div_td_target = rnd_r + self.gamma * self.dv(s_prime) * done_mask\n",
        "                div_delta = div_td_target - self.dv(s)\n",
        "            delta = delta.numpy()\n",
        "            div_delta = div_delta.numpy()\n",
        "            div_advantage_lst=[]\n",
        "            advantage_lst = []\n",
        "            advantage = 0.0\n",
        "            div_advantage = 0.0\n",
        "            for delta_t in delta[::-1]:\n",
        "                advantage = self.gamma * self.lmbda * advantage + delta_t[0]\n",
        "                advantage_lst.append([advantage])\n",
        "            advantage_lst.reverse()\n",
        "            for delta_t in div_delta[::-1]:\n",
        "                div_advantage = self.gamma * self.lmbda * div_advantage + delta_t[0]\n",
        "                div_advantage_lst.append([div_advantage])\n",
        "            div_advantage_lst.reverse()\n",
        "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
        "            div_advantage = torch.tensor(div_advantage_lst, dtype=torch.float)\n",
        "            data_with_adv.append((s, a, r, rnd_r, s_prime, done_mask, old_log_prob, td_target, div_td_target, advantage, div_advantage))\n",
        "\n",
        "        return data_with_adv\n",
        "\n",
        "\n",
        "    def train_net(self):\n",
        "        if len(self.data) == self.minibatch_size * self.buffer_size:\n",
        "            data = self.make_batch()\n",
        "            data = self.calc_advantage(data)\n",
        "\n",
        "            for i in range(self.K_epoch):\n",
        "                for mini_batch in data:\n",
        "                    s, a, r, rnd_r, s_prime, done_mask, old_log_prob, td_target, div_td_target, advantage, div_advantage = mini_batch\n",
        "\n",
        "                    mu, std = self.pi(s, softmax_dim=1)\n",
        "                    dist = Normal(mu, std)\n",
        "                    log_prob = dist.log_prob(a)\n",
        "                    ratio = torch.exp(log_prob - old_log_prob)  # a/b == exp(log(a)-log(b))\n",
        "\n",
        "                    surr1 = ratio * (advantage+div_advantage)\n",
        "                    surr2 = torch.clamp(ratio, 1-self.eps_clip, 1+self.eps_clip) * (advantage+div_advantage)\n",
        "                    loss = -torch.min(surr1, surr2) + (self.v(s) - td_target).pow(2).mean() + (self.dv(s)- div_td_target).pow(2).mean()\n",
        "                    idx = torch.randint(0, s.size()[0], size=((s.size()[0])//8,))\n",
        "                    # print(idx)\n",
        "                    # print(idx.shape)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                      target_output = self.target_network(s_prime[idx])\n",
        "                    predicted_output = self.prediction_network(s_prime[idx])\n",
        "                    # print(target_output.shape)\n",
        "                    # print(predicted_output.shape)\n",
        "                    rnd_loss = (target_output - predicted_output).pow(2).mean()\n",
        "                    # loss += self.entropy_coeff * dist.entropy()\n",
        "                    loss += rnd_loss\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss.mean().backward()\n",
        "                    nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
        "                    self.optimizer.step()\n",
        "                    self.optimization_step += 1\n",
        "\n",
        "def main():\n",
        "    env = gym.make('Pendulum-v1')\n",
        "    model = PPO(config)\n",
        "    score, div_score, best_ext_score = 0.0, 0.0, -10000\n",
        "\n",
        "    print_interval = 20\n",
        "    rollout = []\n",
        "    writer = SummaryWriter(log_dir=f'runs/PPO')\n",
        "\n",
        "    for n_epi in range(10000):\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        count = 0\n",
        "        episodic_score, episodic_int_score = 0.0, 0.0\n",
        "        while count < 200 and not done:\n",
        "            for t in range(model.rollout_len):\n",
        "                mu, std = model.pi(torch.from_numpy(s).float())\n",
        "                dist = Normal(mu, std)\n",
        "                a = dist.sample()\n",
        "                log_prob = dist.log_prob(a)\n",
        "                s_prime, r, done, truncated, info = env.step([a.item()])\n",
        "                with torch.no_grad():\n",
        "                  int_r = (model.target_network(torch.from_numpy(s_prime).float())- model.prediction_network(torch.from_numpy(s_prime).float())).pow(2).mean()\n",
        "                # print(int_r.item())\n",
        "                # print(r)\n",
        "                rollout.append((s, a, (r/100.0, int_r.item()*10), s_prime, log_prob.item(), done))\n",
        "                if len(rollout) == model.rollout_len:\n",
        "                    model.put_data(rollout)\n",
        "                    rollout = []\n",
        "\n",
        "                s = s_prime\n",
        "                score += r\n",
        "                episodic_score+=r\n",
        "                div_score += int_r.item()\n",
        "                episodic_int_score+=int_r.item()\n",
        "                count += 1\n",
        "\n",
        "            model.train_net()\n",
        "        writer.add_scalar(\"Extrinsic Reward\", episodic_score/count, n_epi)\n",
        "        writer.add_scalar(\"Intrinsic_Reward\", episodic_int_score/count, n_epi)\n",
        "\n",
        "        if n_epi%print_interval==0 and n_epi!=0:\n",
        "            cur_ext_score = score/print_interval\n",
        "            cur_div_score = div_score/print_interval\n",
        "            print(\"# of episode :{}, avg score : {:.1f}, , adv_div_score: {:.4f}, optmization step: {}\".format(n_epi, cur_ext_score, cur_div_score, model.optimization_step))\n",
        "            score = 0.0\n",
        "            div_score = 0.0\n",
        "        if n_epi > 100 and cur_ext_score > best_ext_score:\n",
        "            torch.save(model, \"./best_model.pt\")\n",
        "            best_ext_score = cur_ext_score\n",
        "            print(f\"new model saved. current best score: {best_ext_score} \")\n",
        "        if n_epi%100==0:\n",
        "            torch.save(model, \"./\"+ str(n_epi)+\"_model.pt\")\n",
        "    env.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "9xhtJiIRw3Q_"
      },
      "outputs": [],
      "source": [
        "def play(model):\n",
        "\n",
        "    env = gym.make('Pendulum-v1', render_mode = \"rgb_array\")\n",
        "    env = rl.RenderFrame(env, \"./output\")\n",
        "\n",
        "    obs, info = env.reset(seed=42)\n",
        "    r= 0.0\n",
        "\n",
        "    while True:\n",
        "\n",
        "        a, std = model.pi(torch.from_numpy(obs).float())\n",
        "        \n",
        "        obs, reward, terminated, truncated, info = env.step([a.item()])\n",
        "        r+=reward\n",
        "        if terminated or truncated:\n",
        "            print(r)\n",
        "            break\n",
        "\n",
        "    env.play()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "ZeYoj-eBCg1r",
        "outputId": "f86c9b59-06e0-47c9-eb8b-99d5e3a53d24"
      },
      "outputs": [],
      "source": [
        "model = torch.load(\"best_model.pt\")\n",
        "play(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjDCEvBpyVkl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
